{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted .venv (Python 3.12.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3da75-16b5-49a6-8eaa-c896c3d70f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import context\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "# Third-party\n",
    "from openai import OpenAI\n",
    "import opentimelineio as otio\n",
    "from google import genai\n",
    "\n",
    "# Local imports\n",
    "from models.data_models import ClipSpec, SourceMedia, ClipsList\n",
    "from create_timelines.otio_builder import PerMediaTimelineBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c20f60-69a4-45f9-9e1a-f71bc72e03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9fafa-42f3-4ccb-85e6-8fd3d6c93214",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL_NAME = \"gpt-5.1\"\n",
    "GOOGLE_MODEL_NAME = \"gemini-3-pro-preview\"\n",
    "\n",
    "# Input Paths\n",
    "TRANSCRIPT_PATH = Path(\n",
    "    \"/Users/nico/hack/gitHub/automate-timelines/data/transcripts/002-hwei-only-audio-pre-processing_eng.txt\"\n",
    ")\n",
    "\n",
    "CONTEXT = \"This is a podcast interview between Nicola (the host) and Hwei (the guest). Hwei is a data scientist, AI engineer, and freelance coach. In the interview, she shares her approach to building with AI, reflecting both on her professional experience and her personal journey of growth as she struggled to find the right path for herself. Hwei explains how she now helps others navigate the challenges of starting out as freelancers, sharing the tools and strategies she uses to coach people. The interview has an informal, conversational style between two friends.\"\n",
    "\n",
    "# Video Settings\n",
    "FPS = 24  # Frames per second\n",
    "\n",
    "# Media Files\n",
    "MEDIA_PATHS = [\n",
    "    \"/Users/nico/YT/automation-tests/recording/nicola.mp4\",\n",
    "    \"/Users/nico/YT/automation-tests/recording/hwei.mp4\",\n",
    "]\n",
    "\n",
    "# Output Path\n",
    "OUTPUT_OTIO_PATH = Path(\n",
    "    \"/Users/nico/hack/gitHub/automate-timelines/data/timelines/timeline_elevenlabs_gpt51.otio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba21dc-54ac-4f85-b315-0bd19af7d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:58:40 - INFO - Loading transcript from /Users/nico/hack/gitHub/automate-timelines/data/transcripts/002-hwei-only-audio-pre-processing_eng.txt\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# AI PROMPT\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "You will analyze a podcast transcript to identify and extract 3-4 compelling segments that can be used to create a 2-minute teaser introduction for the episode. Your goal is to select segments that will hook listeners and motivate them to listen to the full episode.\n",
    "\n",
    "## Input Materials\n",
    "\n",
    "First, here is additional context about the podcast show, host, and guest:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the full podcast transcript you will analyze:\n",
    "\n",
    "<transcript>\n",
    "{transcript}\n",
    "</transcript>\n",
    "\n",
    "The transcript follows this format:\n",
    "- Each segment begins with a line containing two timestamps: start time --> end time (format: HH:MM:SS,mmm)\n",
    "- The speaker's name appears in square brackets on the same line as the timestamps\n",
    "- The spoken text appears on the following line(s)\n",
    "\n",
    "Example:\n",
    "```\n",
    "00:00:16,640 --> 00:00:18,820 [John]\n",
    "and I really look forward to our good\n",
    "\n",
    "00:00:18,860 --> 00:00:19,900 [John]\n",
    "conversation today.\n",
    "```\n",
    "\n",
    "## Your Task\n",
    "\n",
    "Select 3-4 specific segments from the transcript that will work best as teaser content for a 2-minute podcast introduction. The combined duration of your selected segments should total approximately 120 seconds or less.\n",
    "\n",
    "## What Makes a Great Teaser Segment\n",
    "\n",
    "Select segments that accomplish one or more of these goals:\n",
    "\n",
    "1. **Hook listeners immediately** - Surprising, intriguing, or dramatic statements that grab attention\n",
    "2. **Create cliffhangers** - Incomplete thoughts, questions raised without answers, stories that build tension\n",
    "3. **Showcase compelling content** - Revelations, insights, emotional moments, demonstrations of expertise, unique perspectives\n",
    "4. **Generate curiosity** - Statements that make listeners want to know more\n",
    "\n",
    "## Selection Criteria\n",
    "\n",
    "Prioritize segments with these qualities:\n",
    "- Surprising revelations or unexpected insights\n",
    "- Emotional or dramatic moments\n",
    "- Statements that raise intriguing questions without providing complete answers\n",
    "- Moments that demonstrate the guest's unique expertise or perspective\n",
    "- Story beginnings that create suspense\n",
    "- Controversial or thought-provoking statements\n",
    "\n",
    "## Analysis Process\n",
    "\n",
    "Before providing your final output, work through your analysis in <segment_analysis> tags. In this section:\n",
    "\n",
    "1. **Parse the transcript format**: Confirm you understand the timestamp format (HH:MM:SS,mmm) and speaker labels so you can accurately extract segments\n",
    "\n",
    "2. **Scan for potential segments**: Go through the transcript and identify potential teaser segments. For each one, write out:\n",
    "   - The start timestamp\n",
    "   - The end timestamp\n",
    "   - The exact text verbatim from the transcript\n",
    "   \n",
    "   It's OK for this section to be quite long - take your time to identify all promising candidates.\n",
    "\n",
    "3. **Evaluate each candidate**: For each potential segment you identified, systematically assess it against the four main criteria. Be explicit:\n",
    "   - Does it hook listeners immediately? (Answer yes or no, then explain why)\n",
    "   - Does it create cliffhangers? (Answer yes or no, then explain why)\n",
    "   - Does it showcase compelling content? (Answer yes or no, then explain why)\n",
    "   - Does it generate curiosity? (Answer yes or no, then explain why)\n",
    "\n",
    "4. **Calculate durations**: For each promising segment, calculate the duration in seconds. Show your work:\n",
    "   - Convert start timestamp to total seconds\n",
    "   - Convert end timestamp to total seconds\n",
    "   - Subtract to get duration\n",
    "   - Example: 00:01:30,000 = (0*3600) + (1*60) + 30 = 90 seconds\n",
    "\n",
    "5. **Consider combinations**: Think about how different segments might work together as a cohesive teaser that flows well and creates a compelling narrative arc\n",
    "\n",
    "6. **Verify total timing**: Add up the durations of your candidate segments step-by-step, showing the math (e.g., \"Segment 1: 25 seconds + Segment 2: 18 seconds + Segment 3: 32 seconds = 75 seconds total\"). Ensure the total is approximately 120 seconds or less\n",
    "\n",
    "7. **Make final selection**: Choose your final 3-4 segments that work best together, meet the timing constraint, and maximize teaser impact\n",
    "\n",
    "Take your time with this analysis section - it's important to thoroughly review the entire transcript and show your reasoning.\n",
    "\n",
    "## Output Format\n",
    "\n",
    "After completing your analysis, provide your final selection as a JSON array. Each object in the array should represent one selected segment with these exact fields:\n",
    "\n",
    "- `start`: Start timestamp in 'HH:MM:SS,mmm' format (e.g., \"01:23:48,320\")\n",
    "- `end`: End timestamp in 'HH:MM:SS,mmm' format (e.g., \"01:23:53,639\")\n",
    "- `transcript_text`: The exact text from the transcript for this segment\n",
    "- `notes`: A brief explanation of why you selected this segment\n",
    "\n",
    "Output your final selection in <clips> tags in this format\n",
    "\n",
    "# ClipsList\n",
    "## ClipSelection 1\n",
    "    - \"start\": \"HH:MM:SS,mmm\",\n",
    "    - \"end\": \"HH:MM:SS,mmm\",\n",
    "    - \"transcript_text\": \"exact text from transcript\",\n",
    "    - \"notes\": \"brief explanation of teaser value\"\n",
    "## ClipSelection 2\n",
    "    - \"start\": \"HH:MM:SS,mmm\",\n",
    "    - \"end\": \"HH:MM:SS,mmm\",\n",
    "    - \"transcript_text\": \"exact text from transcript\",\n",
    "    - \"notes\": \"brief explanation of teaser value\"\n",
    "]\n",
    "```\n",
    "\n",
    "Include 3-4 objects in your response depending on how many segments best fit within the 2-minute constraint while maximizing teaser impact.\n",
    "\n",
    "## Important Reminders\n",
    "\n",
    "- Extract timestamps and text exactly as they appear in the transcript\n",
    "- Ensure your selected segments combine to approximately 120 seconds or less\n",
    "- Your JSON output should contain only your final selections, not any of the analysis work\n",
    "- Make sure your JSON is valid and properly formatted\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN EXECUTION\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Step 1: Load and process transcript\n",
    "logger.info(f\"Loading transcript from {TRANSCRIPT_PATH}\")\n",
    "transcript = TRANSCRIPT_PATH.read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a137e-68a1-4110-b2b5-2c96706147a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446d60e-c1f6-43b4-b206-40dc84a67648",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m response = google_client.models.generate_content(\n\u001b[32m      2\u001b[39m     model=GOOGLE_MODEL_NAME,\n\u001b[32m      3\u001b[39m     contents=ORCHESTRATOR_PROMPT.format(transcript=transcript, context=CONTEXT),\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     config=\u001b[43mtypes\u001b[49m.GenerateContentConfig(\n\u001b[32m      5\u001b[39m         thinking_config=types.ThinkingConfig(thinking_level=\u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m         response_mime_type=\u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m         response_json_schema=ClipsList.model_json_schema(),\n\u001b[32m      8\u001b[39m     ),\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'types' is not defined"
     ]
    }
   ],
   "source": [
    "response = google_client.models.generate_content(\n",
    "    model=GOOGLE_MODEL_NAME,\n",
    "    contents=ORCHESTRATOR_PROMPT.format(transcript=transcript, context=CONTEXT),\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_level=\"low\"),\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_json_schema=ClipsList.model_json_schema(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a17c7-7209-4961-ad84-0cc0513bbeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765c850-2a4e-472d-b2ad-866441897802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:59:33 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-12-10 11:06:05 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-preview:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = google_client.models.generate_content(\n",
    "    model=GOOGLE_MODEL_NAME,\n",
    "    contents=ORCHESTRATOR_PROMPT.format(transcript=transcript, context=CONTEXT),\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_level=\"low\"),\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_json_schema=ClipsList.model_json_schema(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95c565-a960-4b8c-b444-139420d5c954",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ClipsList\n  JSON input should be string, bytes or bytearray [type=json_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m clips_list = \u001b[43mClipsList\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hack/gitHub/automate-timelines/.venv/lib/python3.12/site-packages/pydantic/main.py:766\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, extra, context, by_alias, by_name)\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    761\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    762\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    763\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    764\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ClipsList\n  JSON input should be string, bytes or bytearray [type=json_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/json_type"
     ]
    }
   ],
   "source": [
    "clips_list = ClipsList.model_validate_json(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1713ad0-4407-4b1f-8abb-962554411996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-3-pro-preview',\n",
       "  response_id='DUY5af6JBLnhnsEP7ZuguAQ',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    cache_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=85603\n",
       "      ),\n",
       "    ],\n",
       "    cached_content_token_count=85603,\n",
       "    prompt_token_count=85952,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=85952\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=1274,\n",
       "    total_token_count=87226\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7ab4c-c2e3-4a21-9039-9d872fb9cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027cbe6-7cdc-49f2-8708-c9744767237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
